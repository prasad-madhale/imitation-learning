{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C\n",
    "\n",
    "Actor-Critic Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self, name : str, obs_space, action_space, sess):\n",
    "        self.observation = obs_space\n",
    "        self.action = action_space\n",
    "        self.scope_name = name\n",
    "        self.sess = sess\n",
    "        \n",
    "        with(tf.variable_scope(self.scope_name)):\n",
    "            # placeholder for inputs to the network\n",
    "            self.inputs = tf.placeholder(shape = [None] + list(self.observation.shape) , dtype = tf.float32)\n",
    "        \n",
    "            # build the two networks\n",
    "            self.build_network()\n",
    "            \n",
    "            # stochastic action\n",
    "            self.act = tf.multinomial(tf.log(self.act_probs),1)\n",
    "            self.act = tf.reshape(self.act, shape = [-1])\n",
    "            \n",
    "    def build_network(self):\n",
    "        # critic network gives out value prediction for the given inputs\n",
    "        with tf.variable_scope('critic', reuse = tf.AUTO_REUSE):\n",
    "            cout = tf.layers.dense(self.inputs, units = 16, activation = tf.tanh)\n",
    "            cout = tf.layers.dense(cout, units = 32, activation = tf.tanh)\n",
    "            self.value = tf.layers.dense(cout, units = 1, activation = None)\n",
    "        \n",
    "        # actor network spits out action probabilites\n",
    "        with tf.variable_scope('actor', reuse = tf.AUTO_REUSE):\n",
    "            aout = tf.layers.dense(self.inputs, 16,activation = tf.tanh)\n",
    "            aout = tf.layers.dense(aout, units = 32, activation = tf.tanh)\n",
    "            aout = tf.layers.dense(aout, units = 16, activation = tf.tanh)\n",
    "            self.act_probs = tf.layers.dense(aout, self.action.n , activation = tf.nn.softmax)\n",
    "                    \n",
    "    # get actions based on the given inputs\n",
    "    def get_action(self, inputs):\n",
    "        return self.sess.run(self.act, feed_dict = {self.inputs : inputs})\n",
    "    \n",
    "    # get value prediction for the given inputs\n",
    "    def get_value(self, inputs):\n",
    "        return self.sess.run(self.value, feed_dict = {self.inputs : inputs})\n",
    "    \n",
    "    # get all trainable variables required for policy update later\n",
    "    def trainable_vars(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,self.scope_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \n",
    "    def __init__(self, env, sess, eps = 0.2, gamma = 0.95, clip1=1, clip2=0.01, learning_rate = 5e-5):\n",
    "        self.sess = sess\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.clip1 = clip1\n",
    "        self.clip2 = clip2\n",
    "        self.act_clip_max = 1\n",
    "        self.act_clip_min = 1e-10\n",
    "        \n",
    "        self.pi = A2C(\"pi\", env.observation_space, env.action_space, self.sess)\n",
    "        self.old_pi = A2C(\"old_pi\", env.observation_space, env.action_space, self.sess)\n",
    "        \n",
    "        self.pi_trainable_params = self.pi.trainable_vars()\n",
    "        self.old_pi_trainable_params = self.old_pi.trainable_vars()\n",
    "        \n",
    "        with tf.variable_scope('update_policy'):\n",
    "            self.update_ops = [old_pi_vals.assign(pi_vals) for pi_vals, old_pi_vals in zip(self.pi_trainable_params, self.old_pi_trainable_params)]\n",
    "        \n",
    "        with tf.variable_scope('training_inputs'):\n",
    "            self.actions = tf.placeholder(shape = [None], dtype=tf.int32)\n",
    "            self.rewards = tf.placeholder(shape = [None], dtype=tf.float32)\n",
    "            self.v_next = tf.placeholder(shape = [None], dtype=tf.float32)\n",
    "            self.adv = tf.placeholder(shape = [None], dtype=tf.float32)\n",
    "            \n",
    "        act_probs = self.hotify_action(self.pi.act_probs)\n",
    "        act_old_probs = self.hotify_action(self.old_pi.act_probs)\n",
    "            \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "        \n",
    "            # loss calculations\n",
    "            clipped_act_probs = tf.log(tf.clip_by_value(act_probs, self.act_clip_min, self.act_clip_max))\n",
    "            clipped_old_act_probs = tf.log(tf.clip_by_value(act_old_probs, self.act_clip_min, self.act_clip_max))\n",
    "            \n",
    "            ratio = tf.exp(clipped_act_probs - clipped_old_act_probs)\n",
    "    \n",
    "            clipped_ratio = tf.clip_by_value(ratio, 1 -self.eps, 1 + self.eps)\n",
    "            surrogate = tf.multiply(ratio, self.adv)\n",
    "            surrogate_clipped = tf.multiply(clipped_ratio, self.adv)\n",
    "            \n",
    "            clipped_loss = tf.minimum(surrogate, surrogate_clipped)\n",
    "            clipped_loss = tf.reduce_mean(clipped_loss)\n",
    "            \n",
    "            entropy = -tf.reduce_sum(self.pi.act_probs * tf.log(tf.clip_by_value(self.pi.act_probs, self.act_clip_min, self.act_clip_max)), 1)\n",
    "            entropy = tf.reduce_mean(entropy, 0)\n",
    "            \n",
    "            value = self.pi.value\n",
    "            error = self.rewards + self.gamma * self.v_next\n",
    "            loss_value = tf.squared_difference(error, value)\n",
    "            loss_value = tf.reduce_sum(loss_value)\n",
    "            \n",
    "            self.loss = -(clipped_loss - self.clip1 * loss_value + self.clip2 * entropy)\n",
    "            self.loss_plot = tf.summary.scalar('loss', self.loss)\n",
    "        \n",
    "        opt = tf.train.AdamOptimizer(self.learning_rate, epsilon=1e-5)\n",
    "        self.gradients = opt.compute_gradients(self.loss, var_list = self.pi_trainable_params)\n",
    "        self.train_op = opt.minimize(self.loss, var_list = self.pi_trainable_params)\n",
    "\n",
    "    # get action given state\n",
    "    def get_action(self, inputs):\n",
    "        return self.pi.get_action(inputs)\n",
    "\n",
    "    # get value estimate given state\n",
    "    def get_value(self, inputs):\n",
    "        return self.pi.get_value(inputs)\n",
    "    \n",
    "    # update old policy network to the new network parameters    \n",
    "    def update_old_policy(self):\n",
    "        self.sess.run(self.update_ops)\n",
    "    \n",
    "    def train_policy(self, inputs, actions, rewards, v_next, advantages):\n",
    "        self.sess.run(self.train_op, feed_dict = {self.pi.inputs : inputs,\n",
    "                                                  self.old_pi.inputs : inputs,\n",
    "                                                  self.actions: actions,\n",
    "                                                  self.rewards: rewards,\n",
    "                                                  self.v_next: v_next, \n",
    "                                                  self.adv: advantages})\n",
    "    \n",
    "    # get advantage estimates\n",
    "    def get_gaes(self, rewards, v_preds, v_preds_next):\n",
    "        deltas = [r_t + self.gamma * v_next - v for r_t, v_next, v in zip(rewards, v_preds_next, v_preds)]\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(gaes) - 1)):\n",
    "            gaes[t] = gaes[t] + self.gamma * gaes[t + 1]\n",
    "        return gaes\n",
    "    \n",
    "    def hotify_action(self, action):\n",
    "        action *= tf.one_hot(self.actions, action.shape[1])\n",
    "        action = tf.reduce_sum(action, 1)\n",
    "        return action\n",
    "    \n",
    "    def get_entropy(self, act_probs):\n",
    "        entropy = -tf.reduce_sum(act_probs * tf.log(tf.clip_by_value(act_probs, self.act_clip_min, self.act_clip_max)), 1)\n",
    "        return tf.reduce_mean(entropy, 0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(num_epochs, ppo, obs, actions, adv, rewards, v_preds_next):\n",
    "    transitions = [obs, actions, adv, rewards, v_preds_next]\n",
    "    \n",
    "    for epochs in range(num_epochs):\n",
    "        # random sampling\n",
    "        index = indices = np.random.randint(0, obs.shape[0], size = 32)\n",
    "        samples = [np.take(transition, index, axis=0) for transition in transitions]\n",
    "\n",
    "        # training\n",
    "        ppo.train_policy(inputs = samples[0],\n",
    "                     actions = samples[1],\n",
    "                     advantages = samples[2],\n",
    "                     rewards = samples[3],\n",
    "                     v_next = samples[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input):\n",
    "    return np.stack([input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(input):\n",
    "    return (input - input.mean())/input.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 5000\n",
    "num_of_epochs = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number: 0\n",
      "Episode number: 1000\n",
      "Episode number: 2000\n",
      "Episode number: 3000\n",
      "Episode number: 4000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tensor_plot = tf.summary.FileWriter('log/ppo', graph = sess.graph)\n",
    "    env = gym.make('CartPole-v0')\n",
    "    ppo = PPO(env, sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    state = env.reset()\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        length = 0\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "                print('Episode number: {}'.format(i))\n",
    "             \n",
    "        while True:\n",
    "            length += 1\n",
    "            \n",
    "            state = preprocess(state)\n",
    "            \n",
    "            action = ppo.get_action(state)\n",
    "            action = np.asscalar(action)\n",
    "            \n",
    "            value = ppo.get_value(state)\n",
    "            value = np.asscalar(value)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            obs.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "               \n",
    "            if done:\n",
    "                next_state = preprocess(next_state)\n",
    "                next_value = ppo.get_value(next_state)\n",
    "                next_value = np.asscalar(next_value)\n",
    "                v_preds_next = values[1:] + [next_value]\n",
    "                state = env.reset()\n",
    "                break\n",
    "            else:\n",
    "                state = next_state\n",
    "        \n",
    "        tensor_plot.add_summary(tf.Summary(value = [tf.Summary.Value(tag=\"my_ppo_episode_rewards\", simple_value = sum(rewards))]), i)\n",
    "        tensor_plot.add_summary(tf.Summary(value = [tf.Summary.Value(tag=\"my_ppo_episode_length\", simple_value = length)]), i)\n",
    "        \n",
    "        adv = ppo.get_gaes(rewards, values, v_preds_next)\n",
    "\n",
    "        obs = np.reshape(obs, newshape=(-1,) + env.observation_space.shape)\n",
    "        \n",
    "        rewards = np.array(rewards)\n",
    "        v_preds_next = np.array(v_preds_next)\n",
    "        actions = np.array(actions)\n",
    "        adv = z_score(np.array(adv))\n",
    "                \n",
    "        ppo.update_old_policy()\n",
    "\n",
    "        epoch_train(num_of_epochs, ppo, obs, actions, adv, rewards, v_preds_next)\n",
    "        \n",
    "    tensor_plot.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines Implementation of PPO\n",
    "\n",
    "From the stable-baselines repo to compare my implementation with the baselines implementation of PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c119359f1709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"log/ppo/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines'"
     ]
    }
   ],
   "source": [
    "from stable_baselines import PPO1\n",
    "\n",
    "model = PPO1('MlpPolicy', 'CartPole-v0', verbose=1, tensorboard_log=\"log/ppo/\")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines Implementation of TRPO\n",
    "\n",
    "From the stable-baselines repo to compare my implementation with the baselines implementation of TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating environment from the given name, wrapped in a DummyVecEnv.\n",
      "********** Iteration 0 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.518 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.109 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0187          0\n",
      "         1   4.86e-05      0.104\n",
      "         2   3.31e-05      0.346\n",
      "         3   2.52e-05      0.363\n",
      "         4   1.08e-07       0.49\n",
      "         5   1.79e-06       0.49\n",
      "         6    1.1e-09      0.492\n",
      "         7   2.73e-11      0.492\n",
      "\u001b[35mdone in 0.154 seconds\u001b[0m\n",
      "Expected: 0.019 Actual: 0.018\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.076 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 21.1        |\n",
      "| EpRewMean       | 21.1        |\n",
      "| EpThisIter      | 48          |\n",
      "| EpisodesSoFar   | 48          |\n",
      "| TimeElapsed     | 0.936       |\n",
      "| TimestepsSoFar  | 1024        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.6851318   |\n",
      "| ev_tdlam_before | -0.00177    |\n",
      "| meankl          | 0.008057453 |\n",
      "| optimgain       | 0.018265154 |\n",
      "| surrgain        | 0.018265154 |\n",
      "---------------------------------\n",
      "********** Iteration 1 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.475 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.000 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0228          0\n",
      "         1   0.000535      0.111\n",
      "         2   0.000689      0.139\n",
      "         3   9.11e-05      0.513\n",
      "         4   1.49e-06      0.739\n",
      "         5   3.16e-07      0.747\n",
      "         6   9.79e-07      0.747\n",
      "         7   5.03e-08       0.75\n",
      "         8   1.58e-08       0.75\n",
      "         9   6.82e-11       0.75\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.024 Actual: 0.028\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.050 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 23.6        |\n",
      "| EpRewMean       | 23.6        |\n",
      "| EpThisIter      | 42          |\n",
      "| EpisodesSoFar   | 90          |\n",
      "| TimeElapsed     | 1.5         |\n",
      "| TimestepsSoFar  | 2050        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.663458    |\n",
      "| ev_tdlam_before | 0.00491     |\n",
      "| meankl          | 0.009241743 |\n",
      "| optimgain       | 0.027608946 |\n",
      "| surrgain        | 0.027608946 |\n",
      "---------------------------------\n",
      "********** Iteration 2 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.562 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0479          0\n",
      "         1    0.00356      0.145\n",
      "         2    0.00186      0.274\n",
      "         3   0.000362      0.316\n",
      "         4   3.16e-05      0.555\n",
      "         5   8.82e-06      0.686\n",
      "         6   2.23e-06       0.73\n",
      "         7   2.67e-07       0.73\n",
      "         8   9.44e-07      0.731\n",
      "         9   1.17e-07      0.732\n",
      "        10    3.1e-08      0.733\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.033 Actual: 0.032\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.062 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 34.8        |\n",
      "| EpRewMean       | 34.8        |\n",
      "| EpThisIter      | 27          |\n",
      "| EpisodesSoFar   | 117         |\n",
      "| TimeElapsed     | 2.16        |\n",
      "| TimestepsSoFar  | 3077        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.62818563  |\n",
      "| ev_tdlam_before | -0.00148    |\n",
      "| meankl          | 0.00975647  |\n",
      "| optimgain       | 0.032262627 |\n",
      "| surrgain        | 0.032262627 |\n",
      "---------------------------------\n",
      "********** Iteration 3 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.471 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0275          0\n",
      "         1    0.00193     0.0859\n",
      "         2    0.00281      0.199\n",
      "         3   0.000783      0.306\n",
      "         4   5.44e-05      0.375\n",
      "         5   1.91e-05      0.523\n",
      "         6   3.94e-06      0.597\n",
      "         7   1.15e-05      0.598\n",
      "         8   1.75e-07      0.628\n",
      "         9   7.78e-07      0.629\n",
      "        10   6.56e-08       0.63\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.025 Actual: 0.026\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.052 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 49.2        |\n",
      "| EpRewMean       | 49.2        |\n",
      "| EpThisIter      | 15          |\n",
      "| EpisodesSoFar   | 132         |\n",
      "| TimeElapsed     | 2.71        |\n",
      "| TimestepsSoFar  | 4113        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.6076422   |\n",
      "| ev_tdlam_before | -0.00709    |\n",
      "| meankl          | 0.010938769 |\n",
      "| optimgain       | 0.025824588 |\n",
      "| surrgain        | 0.025824588 |\n",
      "---------------------------------\n",
      "********** Iteration 4 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.457 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.000 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0502          0\n",
      "         1    0.00177      0.137\n",
      "         2   0.000947      0.209\n",
      "         3   0.000252       0.22\n",
      "         4   0.000142      0.431\n",
      "         5   4.12e-05      0.673\n",
      "         6   2.53e-06      0.787\n",
      "         7   3.17e-05       0.79\n",
      "         8   6.98e-06      0.798\n",
      "         9   1.22e-06      0.798\n",
      "        10   1.01e-07      0.803\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.030 Actual: 0.026\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.043 seconds\u001b[0m\n",
      "----------------------------------\n",
      "| EpLenMean       | 59.7         |\n",
      "| EpRewMean       | 59.7         |\n",
      "| EpThisIter      | 15           |\n",
      "| EpisodesSoFar   | 147          |\n",
      "| TimeElapsed     | 3.26         |\n",
      "| TimestepsSoFar  | 5172         |\n",
      "| entloss         | 0.0          |\n",
      "| entropy         | 0.5816072    |\n",
      "| ev_tdlam_before | -0.0317      |\n",
      "| meankl          | 0.0078895725 |\n",
      "| optimgain       | 0.025714602  |\n",
      "| surrgain        | 0.025714602  |\n",
      "----------------------------------\n",
      "********** Iteration 5 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.481 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.007 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0144          0\n",
      "         1    0.00743     0.0554\n",
      "         2   0.000411     0.0839\n",
      "         3   0.000433      0.119\n",
      "         4   9.47e-05      0.267\n",
      "         5   2.59e-05      0.525\n",
      "         6   0.000196      0.558\n",
      "         7    6.2e-06      0.564\n",
      "         8   5.74e-06        0.6\n",
      "         9   6.99e-07      0.601\n",
      "        10   2.18e-07      0.608\n",
      "\u001b[35mdone in 0.017 seconds\u001b[0m\n",
      "Expected: 0.018 Actual: 0.015\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.043 seconds\u001b[0m\n",
      "----------------------------------\n",
      "| EpLenMean       | 77.5         |\n",
      "| EpRewMean       | 77.5         |\n",
      "| EpThisIter      | 7            |\n",
      "| EpisodesSoFar   | 154          |\n",
      "| TimeElapsed     | 3.83         |\n",
      "| TimestepsSoFar  | 6219         |\n",
      "| entloss         | 0.0          |\n",
      "| entropy         | 0.58555114   |\n",
      "| ev_tdlam_before | -0.163       |\n",
      "| meankl          | 0.006005341  |\n",
      "| optimgain       | 0.0151105225 |\n",
      "| surrgain        | 0.0151105225 |\n",
      "----------------------------------\n",
      "********** Iteration 6 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.450 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.000 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0254          0\n",
      "         1    0.00258     0.0405\n",
      "         2   0.000755      0.068\n",
      "         3   0.000164     0.0771\n",
      "         4    9.8e-05      0.303\n",
      "         5   2.57e-05      0.457\n",
      "         6   0.000121      0.523\n",
      "         7   6.51e-06      0.527\n",
      "         8   2.27e-05      0.556\n",
      "         9   2.44e-06      0.558\n",
      "        10   1.08e-06      0.572\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.017 Actual: 0.017\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.046 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 89.8        |\n",
      "| EpRewMean       | 89.8        |\n",
      "| EpThisIter      | 7           |\n",
      "| EpisodesSoFar   | 161         |\n",
      "| TimeElapsed     | 4.37        |\n",
      "| TimestepsSoFar  | 7334        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.5801389   |\n",
      "| ev_tdlam_before | -0.0966     |\n",
      "| meankl          | 0.010764189 |\n",
      "| optimgain       | 0.016814295 |\n",
      "| surrgain        | 0.016814295 |\n",
      "---------------------------------\n",
      "********** Iteration 7 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.474 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.000 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0353          0\n",
      "         1    0.00203     0.0259\n",
      "         2    0.00142      0.111\n",
      "         3   0.000631      0.167\n",
      "         4   0.000177      0.209\n",
      "         5   4.24e-05      0.418\n",
      "         6   0.000234      0.422\n",
      "         7   9.05e-06      0.485\n",
      "         8   7.66e-07      0.516\n",
      "         9   5.46e-06      0.526\n",
      "        10   2.45e-06      0.526\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.018 Actual: 0.012\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.053 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 106         |\n",
      "| EpRewMean       | 106         |\n",
      "| EpThisIter      | 6           |\n",
      "| EpisodesSoFar   | 167         |\n",
      "| TimeElapsed     | 4.94        |\n",
      "| TimestepsSoFar  | 8409        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.5758752   |\n",
      "| ev_tdlam_before | -0.297      |\n",
      "| meankl          | 0.004431309 |\n",
      "| optimgain       | 0.012316832 |\n",
      "| surrgain        | 0.012316832 |\n",
      "---------------------------------\n",
      "********** Iteration 8 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.473 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0    0.00336          0\n",
      "         1    0.00403     0.0268\n",
      "         2    0.00272      0.187\n",
      "         3   0.000411      0.244\n",
      "         4   0.000919      0.283\n",
      "         5   3.88e-05      0.587\n",
      "         6   2.08e-05      0.643\n",
      "         7   1.11e-05      0.643\n",
      "         8   2.61e-06       0.69\n",
      "         9   2.58e-05      0.701\n",
      "        10   1.25e-07      0.705\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.020 Actual: 0.019\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.040 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 124         |\n",
      "| EpRewMean       | 124         |\n",
      "| EpThisIter      | 5           |\n",
      "| EpisodesSoFar   | 172         |\n",
      "| TimeElapsed     | 5.48        |\n",
      "| TimestepsSoFar  | 9542        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.58651465  |\n",
      "| ev_tdlam_before | -0.385      |\n",
      "| meankl          | 0.010802735 |\n",
      "| optimgain       | 0.018791882 |\n",
      "| surrgain        | 0.018791882 |\n",
      "---------------------------------\n",
      "********** Iteration 9 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.477 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.000 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0   0.000924          0\n",
      "         1    0.00196     0.0243\n",
      "         2    0.00132     0.0662\n",
      "         3    0.00159      0.326\n",
      "         4   0.000122      0.503\n",
      "         5   2.29e-05      0.634\n",
      "         6    0.00035      0.682\n",
      "         7    6.9e-06      0.691\n",
      "         8   4.69e-06      0.714\n",
      "         9   1.82e-05      0.717\n",
      "        10    1.8e-07      0.729\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.017 Actual: 0.009\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.051 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 138         |\n",
      "| EpRewMean       | 138         |\n",
      "| EpThisIter      | 7           |\n",
      "| EpisodesSoFar   | 179         |\n",
      "| TimeElapsed     | 6.03        |\n",
      "| TimestepsSoFar  | 10758       |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.5784534   |\n",
      "| ev_tdlam_before | -0.316      |\n",
      "| meankl          | 0.008309755 |\n",
      "| optimgain       | 0.008746282 |\n",
      "| surrgain        | 0.008746282 |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.trpo_mpi.trpo_mpi.TRPO at 0x1e422e929e8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines import TRPO\n",
    "\n",
    "model = TRPO('MlpPolicy', 'CartPole-v0', verbose=1, tensorboard_log=\"log/trpo/\")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Tensorboard for plots\n",
    "\n",
    "1. Change directory into this project directory\n",
    "2. Execute the following command\n",
    "    `tensorboard --logdir=log/`\n",
    "3. Visit the localhost page with the provided port number to monitor tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
