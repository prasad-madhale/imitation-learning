{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C\n",
    "\n",
    "Actor-Critic Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self, name : str, obs_space, action_space, sess):\n",
    "        self.observation = obs_space\n",
    "        self.action = action_space\n",
    "        self.scope_name = name\n",
    "        self.sess = sess\n",
    "        \n",
    "        with(tf.variable_scope(self.scope_name)):\n",
    "            # placeholder for inputs to the network\n",
    "            self.inputs = tf.placeholder(shape = [None] + list(self.observation.shape) , dtype = tf.float32)\n",
    "        \n",
    "            # build the two networks\n",
    "            self.build_network()\n",
    "            \n",
    "            # stochastic action\n",
    "            self.act = tf.multinomial(tf.log(self.act_probs),1)\n",
    "            self.act = tf.reshape(self.act, shape = [-1])\n",
    "            \n",
    "    def build_network(self):\n",
    "        # critic network gives out value prediction for the given inputs\n",
    "        with tf.variable_scope('critic', reuse = tf.AUTO_REUSE):\n",
    "            cout = tf.layers.dense(self.inputs, units = 16, activation = tf.tanh)\n",
    "            cout = tf.layers.dense(cout, units = 32, activation = tf.tanh)\n",
    "            self.value = tf.layers.dense(cout, units = 1, activation = None)\n",
    "        \n",
    "        # actor network spits out action probabilites\n",
    "        with tf.variable_scope('actor', reuse = tf.AUTO_REUSE):\n",
    "            aout = tf.layers.dense(self.inputs, 16,activation = tf.tanh)\n",
    "            aout = tf.layers.dense(aout, units = 32, activation = tf.tanh)\n",
    "            aout = tf.layers.dense(aout, units = 16, activation = tf.tanh)\n",
    "            self.act_probs = tf.layers.dense(aout, self.action.n , activation = tf.nn.softmax)\n",
    "                    \n",
    "    # get actions based on the given inputs\n",
    "    def get_action(self, inputs):\n",
    "        return self.sess.run(self.act, feed_dict = {self.inputs : inputs})\n",
    "    \n",
    "    # get value prediction for the given inputs\n",
    "    def get_value(self, inputs):\n",
    "        return self.sess.run(self.value, feed_dict = {self.inputs : inputs})\n",
    "    \n",
    "    # get all trainable variables required for policy update later\n",
    "    def trainable_vars(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,self.scope_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \n",
    "    def __init__(self, env, sess, eps = 0.2, gamma = 0.95, clip1=1, clip2=0.01, learning_rate = 5e-5):\n",
    "        self.sess = sess\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.clip1 = clip1\n",
    "        self.clip2 = clip2\n",
    "        self.act_clip_max = 1\n",
    "        self.act_clip_min = 1e-10\n",
    "        \n",
    "        self.pi = A2C(\"pi\", env.observation_space, env.action_space, self.sess)\n",
    "        self.old_pi = A2C(\"old_pi\", env.observation_space, env.action_space, self.sess)\n",
    "        \n",
    "        self.pi_trainable_params = self.pi.trainable_vars()\n",
    "        self.old_pi_trainable_params = self.old_pi.trainable_vars()\n",
    "        \n",
    "        with tf.variable_scope('update_policy'):\n",
    "            self.update_ops = [old_pi_vals.assign(pi_vals) for pi_vals, old_pi_vals in zip(self.pi_trainable_params, self.old_pi_trainable_params)]\n",
    "        \n",
    "        with tf.variable_scope('training_inputs'):\n",
    "            self.actions = tf.placeholder(shape = [None], dtype=tf.int32)\n",
    "            self.rewards = tf.placeholder(shape = [None], dtype=tf.float32)\n",
    "            self.v_next = tf.placeholder(shape = [None], dtype=tf.float32)\n",
    "            self.adv = tf.placeholder(shape = [None], dtype=tf.float32)\n",
    "            \n",
    "        act_probs = self.hotify_action(self.pi.act_probs)\n",
    "        act_old_probs = self.hotify_action(self.old_pi.act_probs)\n",
    "            \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "        \n",
    "            # loss calculations\n",
    "            clipped_act_probs = tf.log(tf.clip_by_value(act_probs, self.act_clip_min, self.act_clip_max))\n",
    "            clipped_old_act_probs = tf.log(tf.clip_by_value(act_old_probs, self.act_clip_min, self.act_clip_max))\n",
    "            \n",
    "            ratio = tf.exp(clipped_act_probs - clipped_old_act_probs)\n",
    "    \n",
    "            clipped_ratio = tf.clip_by_value(ratio, 1 -self.eps, 1 + self.eps)\n",
    "            surrogate = tf.multiply(ratio, self.adv)\n",
    "            surrogate_clipped = tf.multiply(clipped_ratio, self.adv)\n",
    "            \n",
    "            clipped_loss = tf.minimum(surrogate, surrogate_clipped)\n",
    "            clipped_loss = tf.reduce_mean(clipped_loss)\n",
    "            \n",
    "            entropy = -tf.reduce_sum(self.pi.act_probs * tf.log(tf.clip_by_value(self.pi.act_probs, self.act_clip_min, self.act_clip_max)), 1)\n",
    "            entropy = tf.reduce_mean(entropy, 0)\n",
    "            \n",
    "            value = self.pi.value\n",
    "            error = self.rewards + self.gamma * self.v_next\n",
    "            loss_value = tf.squared_difference(error, value)\n",
    "            loss_value = tf.reduce_sum(loss_value)\n",
    "            \n",
    "            self.loss = -(clipped_loss - self.clip1 * loss_value + self.clip2 * entropy)\n",
    "            self.loss_plot = tf.summary.scalar('loss', self.loss)\n",
    "        \n",
    "        opt = tf.train.AdamOptimizer(self.learning_rate, epsilon=1e-5)\n",
    "        self.gradients = opt.compute_gradients(self.loss, var_list = self.pi_trainable_params)\n",
    "        self.train_op = opt.minimize(self.loss, var_list = self.pi_trainable_params)\n",
    "\n",
    "    # get action given state\n",
    "    def get_action(self, inputs):\n",
    "        return self.pi.get_action(inputs)\n",
    "\n",
    "    # get value estimate given state\n",
    "    def get_value(self, inputs):\n",
    "        return self.pi.get_value(inputs)\n",
    "    \n",
    "    # update old policy network to the new network parameters    \n",
    "    def update_old_policy(self):\n",
    "        self.sess.run(self.update_ops)\n",
    "    \n",
    "    def train_policy(self, inputs, actions, rewards, v_next, advantages):\n",
    "        self.sess.run(self.train_op, feed_dict = {self.pi.inputs : inputs,\n",
    "                                                  self.old_pi.inputs : inputs,\n",
    "                                                  self.actions: actions,\n",
    "                                                  self.rewards: rewards,\n",
    "                                                  self.v_next: v_next, \n",
    "                                                  self.adv: advantages})\n",
    "    \n",
    "    # get advantage estimates\n",
    "    def get_gaes(self, rewards, v_preds, v_preds_next):\n",
    "        deltas = [r_t + self.gamma * v_next - v for r_t, v_next, v in zip(rewards, v_preds_next, v_preds)]\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(gaes) - 1)):\n",
    "            gaes[t] = gaes[t] + self.gamma * gaes[t + 1]\n",
    "        return gaes\n",
    "    \n",
    "    def hotify_action(self, action):\n",
    "        action *= tf.one_hot(self.actions, action.shape[1])\n",
    "        action = tf.reduce_sum(action, 1)\n",
    "        return action\n",
    "    \n",
    "    def get_entropy(self, act_probs):\n",
    "        entropy = -tf.reduce_sum(act_probs * tf.log(tf.clip_by_value(act_probs, self.act_clip_min, self.act_clip_max)), 1)\n",
    "        return tf.reduce_mean(entropy, 0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(num_epochs, ppo, obs, actions, adv, rewards, v_preds_next):\n",
    "    transitions = [obs, actions, adv, rewards, v_preds_next]\n",
    "    \n",
    "    for epochs in range(num_epochs):\n",
    "        # random sampling\n",
    "        index = indices = np.random.randint(0, obs.shape[0], size = 32)\n",
    "        samples = [np.take(transition, index, axis=0) for transition in transitions]\n",
    "\n",
    "        # training\n",
    "        ppo.train_policy(inputs = samples[0],\n",
    "                     actions = samples[1],\n",
    "                     advantages = samples[2],\n",
    "                     rewards = samples[3],\n",
    "                     v_next = samples[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input):\n",
    "    return np.stack([input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(input):\n",
    "    return (input - input.mean())/input.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 7000\n",
    "num_of_epochs = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasad/anaconda3/envs/tf/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number: 0\n",
      "Episode number: 1000\n",
      "Episode number: 2000\n",
      "Episode number: 3000\n",
      "Episode number: 4000\n",
      "Episode number: 5000\n",
      "Episode number: 6000\n",
      "Episode number: 7000\n",
      "Model saved at trained_model/ppo.ckpt\n",
      "Training complete. Check the tensorboard for plots\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tensor_plot = tf.summary.FileWriter('log/ppo', graph = sess.graph)\n",
    "    env = gym.make('CartPole-v0')\n",
    "    ppo = PPO(env, sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    state = env.reset()\n",
    "    \n",
    "    # save trained model\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    for i in range(iterations+1):\n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        length = 0\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "                print('Episode number: {}'.format(i))\n",
    "             \n",
    "        while True:\n",
    "            length += 1\n",
    "            \n",
    "            state = preprocess(state)\n",
    "            \n",
    "            action = ppo.get_action(state)\n",
    "            action = np.asscalar(action)\n",
    "            \n",
    "            value = ppo.get_value(state)\n",
    "            value = np.asscalar(value)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            obs.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "               \n",
    "            if done:\n",
    "                next_state = preprocess(next_state)\n",
    "                next_value = ppo.get_value(next_state)\n",
    "                next_value = np.asscalar(next_value)\n",
    "                v_preds_next = values[1:] + [next_value]\n",
    "                state = env.reset()\n",
    "                break\n",
    "            else:\n",
    "                state = next_state\n",
    "        \n",
    "        tensor_plot.add_summary(tf.Summary(value = [tf.Summary.Value(tag=\"my_ppo_episode_rewards\", simple_value = sum(rewards))]), i)\n",
    "        tensor_plot.add_summary(tf.Summary(value = [tf.Summary.Value(tag=\"my_ppo_episode_length\", simple_value = length)]), i)\n",
    "        \n",
    "        adv = ppo.get_gaes(rewards, values, v_preds_next)\n",
    "\n",
    "        obs = np.reshape(obs, newshape=(-1,) + env.observation_space.shape)\n",
    "        \n",
    "        rewards = np.array(rewards)\n",
    "        v_preds_next = np.array(v_preds_next)\n",
    "        actions = np.array(actions)\n",
    "        adv = z_score(np.array(adv))\n",
    "                \n",
    "        ppo.update_old_policy()\n",
    "\n",
    "        epoch_train(num_of_epochs, ppo, obs, actions, adv, rewards, v_preds_next)\n",
    "    \n",
    "    path = saver.save(sess, \"trained_model/ppo.ckpt\")\n",
    "    print(\"Model saved at {}\".format(path))\n",
    "    tensor_plot.close()\n",
    "env.close()\n",
    "\n",
    "print(\"Training complete. Check the tensorboard for plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Expert Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from trained_model/ppo.ckpt\n",
      "Model restored!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "iterations = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    env = gym.make('CartPole-v0')\n",
    "    ppo = PPO(env, sess)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    saver.restore(sess, \"trained_model/ppo.ckpt\")\n",
    "    print(\"Model restored!\")\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for itr in range(iterations):\n",
    "        obs = []\n",
    "        actions = []\n",
    "        length = 0\n",
    "        done = False\n",
    "        \n",
    "        while True:\n",
    "            length += 1\n",
    "            state = preprocess(state)\n",
    "            \n",
    "            action = ppo.get_action(state)\n",
    "            action = np.asscalar(action)\n",
    "            \n",
    "            # take action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            obs.append(state)\n",
    "            actions.append(action)\n",
    "            \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                break\n",
    "            else:\n",
    "                state = next_state\n",
    "        \n",
    "        obs = np.reshape(obs, newshape=[-1] + list(env.observation_space.shape))\n",
    "        actions = np.array(actions)\n",
    "        \n",
    "        np.save(\"trajectories/expert_obs.npy\", obs)\n",
    "        np.save(\"trajectories/expert_actions.npy\", actions)\n",
    "        \n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render trained model (Demo)\n",
    "## Condition for environment to be considered as solved:\n",
    "Considered solved when the average reward is **greater than or equal to 195.0 over 100 consecutive trials**.\n",
    "So, we check our test for the above condition and break the testing once this condition is satisfied.\n",
    "\n",
    "Observe the rewards obtained in the demo on tensorboard under the tag *'test_episode_rewards'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from trained_model/ppo.ckpt\n",
      "Model restored!\n",
      "Episode: 0\n",
      "Episode: 10\n",
      "Episode: 20\n",
      "Episode: 30\n",
      "Episode: 40\n",
      "Episode: 50\n",
      "Episode: 60\n",
      "Episode: 70\n",
      "Episode: 80\n",
      "Episode: 90\n",
      "Episode: 100\n",
      "Episode: 110\n",
      "Episode: 120\n",
      "Episode: 130\n",
      "Episode: 140\n",
      "Solved at Episode: 140\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "success_threshold = 195\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tensor_plot = tf.summary.FileWriter('log/ppo', graph = sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    env = gym.make('CartPole-v0')\n",
    "    ppo = PPO(env, sess)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    succ_runs = 0\n",
    "    \n",
    "    saver.restore(sess, \"trained_model/ppo.ckpt\")\n",
    "    print(\"Model restored!\")\n",
    "            \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "        \n",
    "    for itr in range(501):\n",
    "        rewards = []\n",
    "        \n",
    "        if itr % 10 == 0:\n",
    "            print('Episode: {}'.format(itr))\n",
    "            \n",
    "        while True:\n",
    "            state = preprocess(state)\n",
    "            # if you want to render the solved environment consider uncommenting the time.sleep statement below to facilitate \n",
    "            # slow rendering \n",
    "            # env.render()\n",
    "\n",
    "            action = ppo.get_action(state)\n",
    "            action = np.asscalar(action)\n",
    "\n",
    "            state,reward,done,_ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            # to demonstrate slow rendering\n",
    "            # time.sleep(0.025)\n",
    "            \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                break\n",
    "                \n",
    "        tensor_plot.add_summary(tf.Summary(value = [tf.Summary.Value(tag=\"test_episode_rewards\", simple_value = sum(rewards))]), itr) \n",
    "        \n",
    "        if sum(rewards) >= success_threshold:\n",
    "            succ_runs  += 1\n",
    "            \n",
    "            if succ_runs > 100:\n",
    "                print(\"Solved at Episode: {}\".format(itr))\n",
    "                break\n",
    "        else:\n",
    "            succ_runs = 0\n",
    "                    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator\n",
    "Discriminator presensted by a Generative Adversarial Network to discriminate between the generated trajectory from the expert trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines Implementation of PPO\n",
    "\n",
    "From the stable-baselines repo to compare my implementation with the baselines implementation of PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating environment from the given name, wrapped in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasad/anaconda3/envs/tf/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00178 |      -0.00693 |      78.77592 |       0.00012 |       0.69304\n",
      "     -0.00702 |      -0.00692 |      76.62876 |       0.00090 |       0.69229\n",
      "     -0.01035 |      -0.00690 |      74.69688 |       0.00298 |       0.69027\n",
      "     -0.01465 |      -0.00687 |      72.93570 |       0.00662 |       0.68672\n",
      "Evaluating losses...\n",
      "     -0.01600 |      -0.00684 |      71.61264 |       0.00914 |       0.68429\n",
      "----------------------------------\n",
      "| EpLenMean       | 20.4         |\n",
      "| EpRewMean       | 20.4         |\n",
      "| EpThisIter      | 11           |\n",
      "| EpisodesSoFar   | 11           |\n",
      "| TimeElapsed     | 0.771        |\n",
      "| TimestepsSoFar  | 256          |\n",
      "| ev_tdlam_before | 0.0114       |\n",
      "| loss_ent        | 0.684286     |\n",
      "| loss_kl         | 0.009138748  |\n",
      "| loss_pol_entpen | -0.00684286  |\n",
      "| loss_pol_surr   | -0.016003426 |\n",
      "| loss_vf_loss    | 71.61264     |\n",
      "----------------------------------\n",
      "********** Iteration 1 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00243 |      -0.00684 |     105.85242 |       0.00013 |       0.68416\n",
      "     -0.01118 |      -0.00677 |     104.29965 |       0.00171 |       0.67745\n",
      "     -0.02309 |      -0.00667 |     102.33020 |       0.00625 |       0.66691\n",
      "     -0.02260 |      -0.00656 |      99.92596 |       0.01252 |       0.65599\n",
      "Evaluating losses...\n",
      "     -0.02235 |      -0.00651 |      98.24028 |       0.01599 |       0.65058\n",
      "-----------------------------------\n",
      "| EpLenMean       | 25.1          |\n",
      "| EpRewMean       | 25.1          |\n",
      "| EpThisIter      | 9             |\n",
      "| EpisodesSoFar   | 20            |\n",
      "| TimeElapsed     | 1.17          |\n",
      "| TimestepsSoFar  | 544           |\n",
      "| ev_tdlam_before | 0.0271        |\n",
      "| loss_ent        | 0.65058136    |\n",
      "| loss_kl         | 0.015990093   |\n",
      "| loss_pol_entpen | -0.0065058134 |\n",
      "| loss_pol_surr   | -0.022345208  |\n",
      "| loss_vf_loss    | 98.24028      |\n",
      "-----------------------------------\n",
      "********** Iteration 2 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00072 |      -0.00648 |      92.47142 |       0.00013 |       0.64798\n",
      "     -0.00435 |      -0.00638 |      89.49728 |       0.00116 |       0.63828\n",
      "     -0.00767 |      -0.00627 |      86.04469 |       0.00348 |       0.62714\n",
      "     -0.00647 |      -0.00619 |      81.95870 |       0.00586 |       0.61924\n",
      "Evaluating losses...\n",
      "     -0.00740 |      -0.00618 |      79.24792 |       0.00641 |       0.61805\n",
      "-----------------------------------\n",
      "| EpLenMean       | 25.6          |\n",
      "| EpRewMean       | 25.6          |\n",
      "| EpThisIter      | 9             |\n",
      "| EpisodesSoFar   | 29            |\n",
      "| TimeElapsed     | 1.58          |\n",
      "| TimestepsSoFar  | 811           |\n",
      "| ev_tdlam_before | 0.00507       |\n",
      "| loss_ent        | 0.61804825    |\n",
      "| loss_kl         | 0.0064121517  |\n",
      "| loss_pol_entpen | -0.0061804825 |\n",
      "| loss_pol_surr   | -0.007396441  |\n",
      "| loss_vf_loss    | 79.24792      |\n",
      "-----------------------------------\n",
      "********** Iteration 3 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00034 |      -0.00642 |     125.61749 |      1.45e-05 |       0.64151\n",
      "     -0.00196 |      -0.00644 |     119.04672 |       0.00022 |       0.64420\n",
      "     -0.00378 |      -0.00648 |     111.40587 |       0.00094 |       0.64823\n",
      "     -0.00647 |      -0.00652 |     103.21950 |       0.00216 |       0.65235\n",
      "Evaluating losses...\n",
      "     -0.00744 |      -0.00654 |      97.87889 |       0.00315 |       0.65450\n",
      "-----------------------------------\n",
      "| EpLenMean       | 28.8          |\n",
      "| EpRewMean       | 28.8          |\n",
      "| EpThisIter      | 6             |\n",
      "| EpisodesSoFar   | 35            |\n",
      "| TimeElapsed     | 1.99          |\n",
      "| TimestepsSoFar  | 1094          |\n",
      "| ev_tdlam_before | -0.0966       |\n",
      "| loss_ent        | 0.6544962     |\n",
      "| loss_kl         | 0.0031509665  |\n",
      "| loss_pol_entpen | -0.006544962  |\n",
      "| loss_pol_surr   | -0.0074370056 |\n",
      "| loss_vf_loss    | 97.87889      |\n",
      "-----------------------------------\n",
      "********** Iteration 4 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00043 |      -0.00656 |     140.46295 |      2.24e-05 |       0.65574\n",
      "    -7.83e-06 |      -0.00656 |     128.36957 |      8.68e-05 |       0.65581\n",
      "     -0.00200 |      -0.00653 |     115.86839 |       0.00025 |       0.65250\n",
      "     -0.00524 |      -0.00646 |     103.15985 |       0.00094 |       0.64577\n",
      "Evaluating losses...\n",
      "     -0.00698 |      -0.00640 |      95.54910 |       0.00188 |       0.64019\n",
      "-----------------------------------\n",
      "| EpLenMean       | 31            |\n",
      "| EpRewMean       | 31            |\n",
      "| EpThisIter      | 4             |\n",
      "| EpisodesSoFar   | 39            |\n",
      "| TimeElapsed     | 2.46          |\n",
      "| TimestepsSoFar  | 1365          |\n",
      "| ev_tdlam_before | 0.0543        |\n",
      "| loss_ent        | 0.64018595    |\n",
      "| loss_kl         | 0.0018792218  |\n",
      "| loss_pol_entpen | -0.0064018597 |\n",
      "| loss_pol_surr   | -0.006984737  |\n",
      "| loss_vf_loss    | 95.549095     |\n",
      "-----------------------------------\n",
      "********** Iteration 5 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00136 |      -0.00638 |     118.03637 |       0.00016 |       0.63798\n",
      "     -0.00404 |      -0.00627 |     105.45085 |       0.00128 |       0.62661\n",
      "     -0.00623 |      -0.00619 |      93.55683 |       0.00266 |       0.61912\n",
      "     -0.00837 |      -0.00619 |      83.10535 |       0.00305 |       0.61855\n",
      "Evaluating losses...\n",
      "     -0.01068 |      -0.00621 |      77.17358 |       0.00273 |       0.62148\n",
      "----------------------------------\n",
      "| EpLenMean       | 34.7         |\n",
      "| EpRewMean       | 34.7         |\n",
      "| EpThisIter      | 5            |\n",
      "| EpisodesSoFar   | 44           |\n",
      "| TimeElapsed     | 2.83         |\n",
      "| TimestepsSoFar  | 1693         |\n",
      "| ev_tdlam_before | -0.0977      |\n",
      "| loss_ent        | 0.6214794    |\n",
      "| loss_kl         | 0.002732378  |\n",
      "| loss_pol_entpen | -0.006214794 |\n",
      "| loss_pol_surr   | -0.010683626 |\n",
      "| loss_vf_loss    | 77.173584    |\n",
      "----------------------------------\n",
      "********** Iteration 6 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00021 |      -0.00647 |     131.24051 |       0.00011 |       0.64674\n",
      "     -0.00319 |      -0.00652 |     118.78854 |       0.00091 |       0.65199\n",
      "     -0.00493 |      -0.00656 |     106.59187 |       0.00249 |       0.65596\n",
      "     -0.00507 |      -0.00658 |      96.01835 |       0.00421 |       0.65819\n",
      "Evaluating losses...\n",
      "     -0.00548 |      -0.00659 |      89.94640 |       0.00514 |       0.65876\n",
      "----------------------------------\n",
      "| EpLenMean       | 37.8         |\n",
      "| EpRewMean       | 37.8         |\n",
      "| EpThisIter      | 3            |\n",
      "| EpisodesSoFar   | 47           |\n",
      "| TimeElapsed     | 3.21         |\n",
      "| TimestepsSoFar  | 1957         |\n",
      "| ev_tdlam_before | 0.0927       |\n",
      "| loss_ent        | 0.6587553    |\n",
      "| loss_kl         | 0.0051442916 |\n",
      "| loss_pol_entpen | -0.006587553 |\n",
      "| loss_pol_surr   | -0.005478028 |\n",
      "| loss_vf_loss    | 89.946396    |\n",
      "----------------------------------\n",
      "********** Iteration 7 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00095 |      -0.00652 |     101.88322 |      4.75e-05 |       0.65186\n",
      "     -0.00797 |      -0.00646 |      92.97878 |       0.00060 |       0.64592\n",
      "     -0.01385 |      -0.00635 |      84.34569 |       0.00275 |       0.63522\n",
      "     -0.01540 |      -0.00627 |      78.06207 |       0.00556 |       0.62680\n",
      "Evaluating losses...\n",
      "     -0.01578 |      -0.00625 |      74.48853 |       0.00665 |       0.62455\n",
      "----------------------------------\n",
      "| EpLenMean       | 38.9         |\n",
      "| EpRewMean       | 38.9         |\n",
      "| EpThisIter      | 4            |\n",
      "| EpisodesSoFar   | 51           |\n",
      "| TimeElapsed     | 3.58         |\n",
      "| TimestepsSoFar  | 2229         |\n",
      "| ev_tdlam_before | 0.0537       |\n",
      "| loss_ent        | 0.6245511    |\n",
      "| loss_kl         | 0.006648531  |\n",
      "| loss_pol_entpen | -0.006245511 |\n",
      "| loss_pol_surr   | -0.015777223 |\n",
      "| loss_vf_loss    | 74.488525    |\n",
      "----------------------------------\n",
      "********** Iteration 8 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00029 |      -0.00640 |     125.76375 |      2.59e-05 |       0.63969\n",
      "     -0.00251 |      -0.00637 |     117.38154 |       0.00026 |       0.63673\n",
      "     -0.00525 |      -0.00631 |     109.27007 |       0.00099 |       0.63059\n",
      "     -0.00759 |      -0.00626 |     101.87962 |       0.00226 |       0.62579\n",
      "Evaluating losses...\n",
      "     -0.00943 |      -0.00624 |      97.88159 |       0.00318 |       0.62440\n",
      "-----------------------------------\n",
      "| EpLenMean       | 42.3          |\n",
      "| EpRewMean       | 42.3          |\n",
      "| EpThisIter      | 2             |\n",
      "| EpisodesSoFar   | 53            |\n",
      "| TimeElapsed     | 3.95          |\n",
      "| TimestepsSoFar  | 2548          |\n",
      "| ev_tdlam_before | 0.126         |\n",
      "| loss_ent        | 0.6243992     |\n",
      "| loss_kl         | 0.0031752612  |\n",
      "| loss_pol_entpen | -0.0062439917 |\n",
      "| loss_pol_surr   | -0.00943408   |\n",
      "| loss_vf_loss    | 97.88159      |\n",
      "-----------------------------------\n",
      "********** Iteration 9 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00123 |      -0.00616 |     121.05504 |      5.31e-05 |       0.61594\n",
      "     -0.00660 |      -0.00607 |     113.21112 |       0.00112 |       0.60665\n",
      "     -0.00545 |      -0.00592 |     106.18491 |       0.00543 |       0.59236\n",
      "     -0.00473 |      -0.00586 |      99.69241 |       0.00754 |       0.58562\n",
      "Evaluating losses...\n",
      "     -0.00620 |      -0.00585 |      96.24193 |       0.00652 |       0.58516\n",
      "----------------------------------\n",
      "| EpLenMean       | 43.9         |\n",
      "| EpRewMean       | 43.9         |\n",
      "| EpThisIter      | 2            |\n",
      "| EpisodesSoFar   | 55           |\n",
      "| TimeElapsed     | 4.47         |\n",
      "| TimestepsSoFar  | 2864         |\n",
      "| ev_tdlam_before | 0.214        |\n",
      "| loss_ent        | 0.58516455   |\n",
      "| loss_kl         | 0.0065229246 |\n",
      "| loss_pol_entpen | -0.005851645 |\n",
      "| loss_pol_surr   | -0.006203874 |\n",
      "| loss_vf_loss    | 96.24193     |\n",
      "----------------------------------\n",
      "********** Iteration 10 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00079 |      -0.00603 |     146.00781 |       0.00011 |       0.60323\n",
      "     -0.00410 |      -0.00612 |     135.20543 |       0.00134 |       0.61156\n",
      "     -0.00753 |      -0.00621 |     125.28772 |       0.00458 |       0.62108\n",
      "     -0.00617 |      -0.00626 |     117.38141 |       0.00659 |       0.62614\n",
      "Evaluating losses...\n",
      "     -0.00785 |      -0.00628 |     113.28184 |       0.00680 |       0.62823\n",
      "----------------------------------\n",
      "| EpLenMean       | 49.4         |\n",
      "| EpRewMean       | 49.4         |\n",
      "| EpThisIter      | 2            |\n",
      "| EpisodesSoFar   | 57           |\n",
      "| TimeElapsed     | 4.96         |\n",
      "| TimestepsSoFar  | 3264         |\n",
      "| ev_tdlam_before | 0.271        |\n",
      "| loss_ent        | 0.62823474   |\n",
      "| loss_kl         | 0.006799938  |\n",
      "| loss_pol_entpen | -0.006282348 |\n",
      "| loss_pol_surr   | -0.00785166  |\n",
      "| loss_vf_loss    | 113.28184    |\n",
      "----------------------------------\n",
      "********** Iteration 11 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -4.72e-05 |      -0.00620 |     137.97415 |      9.32e-06 |       0.61989\n",
      "    -1.73e-05 |      -0.00622 |     131.93129 |      9.45e-05 |       0.62173\n",
      "     -0.00080 |      -0.00623 |     126.15787 |       0.00035 |       0.62258\n",
      "     -0.00223 |      -0.00622 |     120.88629 |       0.00078 |       0.62224\n",
      "Evaluating losses...\n",
      "     -0.00302 |      -0.00622 |     117.71505 |       0.00112 |       0.62156\n",
      "-----------------------------------\n",
      "| EpLenMean       | 50.6          |\n",
      "| EpRewMean       | 50.6          |\n",
      "| EpThisIter      | 1             |\n",
      "| EpisodesSoFar   | 58            |\n",
      "| TimeElapsed     | 5.37          |\n",
      "| TimestepsSoFar  | 3520          |\n",
      "| ev_tdlam_before | 0.0543        |\n",
      "| loss_ent        | 0.621557      |\n",
      "| loss_kl         | 0.0011240628  |\n",
      "| loss_pol_entpen | -0.0062155696 |\n",
      "| loss_pol_surr   | -0.003016889  |\n",
      "| loss_vf_loss    | 117.71505     |\n",
      "-----------------------------------\n",
      "********** Iteration 12 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00201 |      -0.00601 |     109.27975 |       0.00019 |       0.60120\n",
      "     -0.01143 |      -0.00589 |     106.72325 |       0.00253 |       0.58859\n",
      "     -0.01045 |      -0.00580 |     104.61842 |       0.00531 |       0.58024\n",
      "     -0.01213 |      -0.00580 |     102.76740 |       0.00549 |       0.57950\n",
      "Evaluating losses...\n",
      "     -0.01317 |      -0.00581 |     101.87941 |       0.00481 |       0.58133\n",
      "----------------------------------\n",
      "| EpLenMean       | 52.5         |\n",
      "| EpRewMean       | 52.5         |\n",
      "| EpThisIter      | 4            |\n",
      "| EpisodesSoFar   | 62           |\n",
      "| TimeElapsed     | 5.77         |\n",
      "| TimestepsSoFar  | 3916         |\n",
      "| ev_tdlam_before | 0.00666      |\n",
      "| loss_ent        | 0.5813285    |\n",
      "| loss_kl         | 0.004809511  |\n",
      "| loss_pol_entpen | -0.005813285 |\n",
      "| loss_pol_surr   | -0.013169894 |\n",
      "| loss_vf_loss    | 101.87941    |\n",
      "----------------------------------\n",
      "********** Iteration 13 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00057 |      -0.00594 |      90.55865 |      2.82e-05 |       0.59367\n",
      "     -0.00378 |      -0.00589 |      89.55319 |       0.00023 |       0.58891\n",
      "     -0.01361 |      -0.00578 |      88.79813 |       0.00126 |       0.57797\n",
      "     -0.01456 |      -0.00569 |      88.09898 |       0.00325 |       0.56873\n",
      "Evaluating losses...\n",
      "     -0.01445 |      -0.00566 |      87.74924 |       0.00438 |       0.56580\n",
      "----------------------------------\n",
      "| EpLenMean       | 53.7         |\n",
      "| EpRewMean       | 53.7         |\n",
      "| EpThisIter      | 4            |\n",
      "| EpisodesSoFar   | 66           |\n",
      "| TimeElapsed     | 6.15         |\n",
      "| TimestepsSoFar  | 4243         |\n",
      "| ev_tdlam_before | 0.00371      |\n",
      "| loss_ent        | 0.5657952    |\n",
      "| loss_kl         | 0.004383201  |\n",
      "| loss_pol_entpen | -0.005657952 |\n",
      "| loss_pol_surr   | -0.014454406 |\n",
      "| loss_vf_loss    | 87.74924     |\n",
      "----------------------------------\n",
      "********** Iteration 14 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00066 |      -0.00598 |     124.75398 |      4.47e-05 |       0.59832\n",
      "     -0.00490 |      -0.00591 |     122.72031 |       0.00085 |       0.59066\n",
      "     -0.00602 |      -0.00582 |     120.33156 |       0.00317 |       0.58247\n",
      "     -0.00680 |      -0.00579 |     117.73050 |       0.00416 |       0.57900\n",
      "Evaluating losses...\n",
      "     -0.00703 |      -0.00580 |     116.19383 |       0.00380 |       0.57951\n",
      "-----------------------------------\n",
      "| EpLenMean       | 56            |\n",
      "| EpRewMean       | 56            |\n",
      "| EpThisIter      | 2             |\n",
      "| EpisodesSoFar   | 68            |\n",
      "| TimeElapsed     | 6.54          |\n",
      "| TimestepsSoFar  | 4542          |\n",
      "| ev_tdlam_before | 0.034         |\n",
      "| loss_ent        | 0.5795088     |\n",
      "| loss_kl         | 0.0037986329  |\n",
      "| loss_pol_entpen | -0.0057950877 |\n",
      "| loss_pol_surr   | -0.0070255883 |\n",
      "| loss_vf_loss    | 116.19383     |\n",
      "-----------------------------------\n",
      "********** Iteration 15 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00276 |      -0.00602 |     132.55930 |      9.09e-05 |       0.60184\n",
      "     5.45e-05 |      -0.00605 |     129.24554 |       0.00026 |       0.60475\n",
      "     -0.00531 |      -0.00608 |     126.15785 |       0.00272 |       0.60799\n",
      "     -0.00624 |      -0.00611 |     122.77010 |       0.00307 |       0.61144\n",
      "Evaluating losses...\n",
      "     -0.00659 |      -0.00614 |     120.87621 |       0.00187 |       0.61391\n",
      "-----------------------------------\n",
      "| EpLenMean       | 58            |\n",
      "| EpRewMean       | 58            |\n",
      "| EpThisIter      | 1             |\n",
      "| EpisodesSoFar   | 69            |\n",
      "| TimeElapsed     | 6.92          |\n",
      "| TimestepsSoFar  | 4831          |\n",
      "| ev_tdlam_before | 0.0177        |\n",
      "| loss_ent        | 0.6139144     |\n",
      "| loss_kl         | 0.0018730757  |\n",
      "| loss_pol_entpen | -0.0061391434 |\n",
      "| loss_pol_surr   | -0.006588445  |\n",
      "| loss_vf_loss    | 120.876205    |\n",
      "-----------------------------------\n",
      "********** Iteration 16 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00086 |      -0.00607 |     123.21065 |       0.00015 |       0.60713\n",
      "     -0.00206 |      -0.00610 |     120.75154 |       0.00180 |       0.60975\n",
      "     -0.00198 |      -0.00614 |     118.23775 |       0.00230 |       0.61394\n",
      "     -0.00372 |      -0.00618 |     116.01583 |       0.00107 |       0.61752\n",
      "Evaluating losses...\n",
      "     -0.00381 |      -0.00619 |     114.68408 |       0.00091 |       0.61880\n",
      "-----------------------------------\n",
      "| EpLenMean       | 61            |\n",
      "| EpRewMean       | 61            |\n",
      "| EpThisIter      | 2             |\n",
      "| EpisodesSoFar   | 71            |\n",
      "| TimeElapsed     | 7.29          |\n",
      "| TimestepsSoFar  | 5181          |\n",
      "| ev_tdlam_before | 0.0187        |\n",
      "| loss_ent        | 0.6187967     |\n",
      "| loss_kl         | 0.0009058843  |\n",
      "| loss_pol_entpen | -0.0061879666 |\n",
      "| loss_pol_surr   | -0.0038141161 |\n",
      "| loss_vf_loss    | 114.68408     |\n",
      "-----------------------------------\n",
      "********** Iteration 17 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00070 |      -0.00604 |     130.30365 |      1.65e-05 |       0.60444\n",
      "      0.00048 |      -0.00605 |     127.61176 |      2.87e-05 |       0.60475\n",
      "     5.32e-05 |      -0.00605 |     124.95543 |      8.98e-06 |       0.60466\n",
      "     -0.00113 |      -0.00604 |     122.30515 |      9.04e-05 |       0.60394\n",
      "Evaluating losses...\n",
      "     -0.00200 |      -0.00603 |     120.72977 |       0.00025 |       0.60301\n",
      "-----------------------------------\n",
      "| EpLenMean       | 62.2          |\n",
      "| EpRewMean       | 62.2          |\n",
      "| EpThisIter      | 1             |\n",
      "| EpisodesSoFar   | 72            |\n",
      "| TimeElapsed     | 7.67          |\n",
      "| TimestepsSoFar  | 5461          |\n",
      "| ev_tdlam_before | 0.00338       |\n",
      "| loss_ent        | 0.6030138     |\n",
      "| loss_kl         | 0.00025104874 |\n",
      "| loss_pol_entpen | -0.006030138  |\n",
      "| loss_pol_surr   | -0.0019973665 |\n",
      "| loss_vf_loss    | 120.72977     |\n",
      "-----------------------------------\n",
      "********** Iteration 18 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00014 |      -0.00599 |     118.34317 |      1.82e-05 |       0.59918\n",
      "     -0.00103 |      -0.00599 |     116.67757 |       0.00024 |       0.59929\n",
      "     -0.00232 |      -0.00600 |     114.95796 |       0.00083 |       0.59959\n",
      "     -0.00268 |      -0.00600 |     113.57232 |       0.00163 |       0.60026\n",
      "Evaluating losses...\n",
      "     -0.00305 |      -0.00601 |     112.62273 |       0.00208 |       0.60081\n",
      "-----------------------------------\n",
      "| EpLenMean       | 63.6          |\n",
      "| EpRewMean       | 63.6          |\n",
      "| EpThisIter      | 2             |\n",
      "| EpisodesSoFar   | 74            |\n",
      "| TimeElapsed     | 8.05          |\n",
      "| TimestepsSoFar  | 5844          |\n",
      "| ev_tdlam_before | -0.00314      |\n",
      "| loss_ent        | 0.60080504    |\n",
      "| loss_kl         | 0.0020832699  |\n",
      "| loss_pol_entpen | -0.0060080504 |\n",
      "| loss_pol_surr   | -0.0030530244 |\n",
      "| loss_vf_loss    | 112.62273     |\n",
      "-----------------------------------\n",
      "********** Iteration 19 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00046 |      -0.00616 |     129.04997 |      4.50e-05 |       0.61603\n",
      "     -0.00024 |      -0.00618 |     127.69974 |       0.00018 |       0.61759\n",
      "     -0.00067 |      -0.00618 |     126.24540 |       0.00056 |       0.61809\n",
      "     -0.00080 |      -0.00618 |     124.81326 |       0.00120 |       0.61816\n",
      "Evaluating losses...\n",
      "     -0.00096 |      -0.00618 |     123.92720 |       0.00123 |       0.61825\n",
      "-----------------------------------\n",
      "| EpLenMean       | 67.1          |\n",
      "| EpRewMean       | 67.1          |\n",
      "| EpThisIter      | 2             |\n",
      "| EpisodesSoFar   | 76            |\n",
      "| TimeElapsed     | 8.45          |\n",
      "| TimestepsSoFar  | 6256          |\n",
      "| ev_tdlam_before | 0.00891       |\n",
      "| loss_ent        | 0.6182549     |\n",
      "| loss_kl         | 0.0012299092  |\n",
      "| loss_pol_entpen | -0.006182549  |\n",
      "| loss_pol_surr   | -0.0009603938 |\n",
      "| loss_vf_loss    | 123.9272      |\n",
      "-----------------------------------\n",
      "********** Iteration 20 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     2.02e-06 |      -0.00597 |     126.84930 |      4.88e-06 |       0.59689\n",
      "     -0.00173 |      -0.00594 |     125.49499 |      9.55e-05 |       0.59438\n",
      "     -0.00403 |      -0.00590 |     124.31490 |       0.00043 |       0.58987\n",
      "     -0.00451 |      -0.00586 |     123.14231 |       0.00091 |       0.58568\n",
      "Evaluating losses...\n",
      "     -0.00457 |      -0.00584 |     122.39738 |       0.00113 |       0.58354\n",
      "-----------------------------------\n",
      "| EpLenMean       | 68.7          |\n",
      "| EpRewMean       | 68.7          |\n",
      "| EpThisIter      | 2             |\n",
      "| EpisodesSoFar   | 78            |\n",
      "| TimeElapsed     | 8.85          |\n",
      "| TimestepsSoFar  | 6533          |\n",
      "| ev_tdlam_before | -0.000432     |\n",
      "| loss_ent        | 0.58353525    |\n",
      "| loss_kl         | 0.0011256912  |\n",
      "| loss_pol_entpen | -0.0058353525 |\n",
      "| loss_pol_surr   | -0.0045723133 |\n",
      "| loss_vf_loss    | 122.39738     |\n",
      "-----------------------------------\n",
      "********** Iteration 21 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00017 |      -0.00595 |     128.47366 |      8.21e-06 |       0.59531\n",
      "     -0.00168 |      -0.00592 |     126.96048 |       0.00015 |       0.59240\n",
      "     -0.00166 |      -0.00590 |     125.26524 |       0.00060 |       0.58981\n",
      "     -0.00174 |      -0.00591 |     123.54427 |       0.00064 |       0.59069\n",
      "Evaluating losses...\n",
      "     -0.00259 |      -0.00593 |     122.56097 |       0.00039 |       0.59318\n",
      "-----------------------------------\n",
      "| EpLenMean       | 70.3          |\n",
      "| EpRewMean       | 70.3          |\n",
      "| EpThisIter      | 1             |\n",
      "| EpisodesSoFar   | 79            |\n",
      "| TimeElapsed     | 9.22          |\n",
      "| TimestepsSoFar  | 6809          |\n",
      "| ev_tdlam_before | -0.00355      |\n",
      "| loss_ent        | 0.5931839     |\n",
      "| loss_kl         | 0.0003888926  |\n",
      "| loss_pol_entpen | -0.0059318393 |\n",
      "| loss_pol_surr   | -0.0025879936 |\n",
      "| loss_vf_loss    | 122.56097     |\n",
      "-----------------------------------\n",
      "********** Iteration 22 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -2.86e-05 |      -0.00594 |     115.24061 |      1.95e-06 |       0.59405\n",
      "     -0.00228 |      -0.00592 |     114.32997 |      6.27e-05 |       0.59211\n",
      "     -0.00502 |      -0.00587 |     113.42428 |       0.00045 |       0.58720\n",
      "     -0.00478 |      -0.00584 |     112.59958 |       0.00098 |       0.58370\n",
      "Evaluating losses...\n",
      "     -0.00473 |      -0.00583 |     112.09595 |       0.00124 |       0.58281\n",
      "-----------------------------------\n",
      "| EpLenMean       | 71.1          |\n",
      "| EpRewMean       | 71.1          |\n",
      "| EpThisIter      | 2             |\n",
      "| EpisodesSoFar   | 81            |\n",
      "| TimeElapsed     | 9.6           |\n",
      "| TimestepsSoFar  | 7141          |\n",
      "| ev_tdlam_before | 0.00106       |\n",
      "| loss_ent        | 0.58281344    |\n",
      "| loss_kl         | 0.0012377846  |\n",
      "| loss_pol_entpen | -0.0058281347 |\n",
      "| loss_pol_surr   | -0.004725743  |\n",
      "| loss_vf_loss    | 112.09595     |\n",
      "-----------------------------------\n",
      "********** Iteration 23 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00055 |      -0.00594 |     128.01187 |      5.71e-06 |       0.59351\n",
      "     -0.00079 |      -0.00593 |     126.80817 |      5.58e-05 |       0.59277\n",
      "     -0.00163 |      -0.00591 |     125.61884 |       0.00032 |       0.59134\n",
      "     -0.00196 |      -0.00593 |     124.35762 |       0.00055 |       0.59261\n",
      "Evaluating losses...\n",
      "     -0.00265 |      -0.00594 |     123.54458 |       0.00053 |       0.59404\n",
      "-----------------------------------\n",
      "| EpLenMean       | 72.7          |\n",
      "| EpRewMean       | 72.7          |\n",
      "| EpThisIter      | 1             |\n",
      "| EpisodesSoFar   | 82            |\n",
      "| TimeElapsed     | 9.99          |\n",
      "| TimestepsSoFar  | 7526          |\n",
      "| ev_tdlam_before | 0.00478       |\n",
      "| loss_ent        | 0.59404314    |\n",
      "| loss_kl         | 0.0005307956  |\n",
      "| loss_pol_entpen | -0.0059404317 |\n",
      "| loss_pol_surr   | -0.002652742  |\n",
      "| loss_vf_loss    | 123.54458     |\n",
      "-----------------------------------\n",
      "********** Iteration 24 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00040 |      -0.00590 |     135.41380 |      3.41e-06 |       0.59048\n",
      "      0.00024 |      -0.00591 |     134.85321 |      7.23e-06 |       0.59097\n",
      "     6.06e-05 |      -0.00589 |     134.30145 |      3.91e-05 |       0.58939\n",
      "     -0.00126 |      -0.00587 |     133.85817 |       0.00013 |       0.58725\n",
      "Evaluating losses...\n",
      "     -0.00141 |      -0.00586 |     133.59760 |       0.00016 |       0.58619\n",
      "-----------------------------------\n",
      "| EpLenMean       | 74.8          |\n",
      "| EpRewMean       | 74.8          |\n",
      "| EpThisIter      | 3             |\n",
      "| EpisodesSoFar   | 85            |\n",
      "| TimeElapsed     | 10.4          |\n",
      "| TimestepsSoFar  | 7967          |\n",
      "| ev_tdlam_before | 0.00107       |\n",
      "| loss_ent        | 0.5861895     |\n",
      "| loss_kl         | 0.00016095144 |\n",
      "| loss_pol_entpen | -0.005861895  |\n",
      "| loss_pol_surr   | -0.0014055967 |\n",
      "| loss_vf_loss    | 133.5976      |\n",
      "-----------------------------------\n",
      "********** Iteration 25 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00036 |      -0.00577 |     127.31430 |      2.14e-06 |       0.57659\n",
      "     -0.00086 |      -0.00576 |     126.53153 |      5.98e-05 |       0.57589\n",
      "     -0.00253 |      -0.00576 |     125.68347 |       0.00021 |       0.57591\n",
      "     -0.00224 |      -0.00576 |     124.82182 |       0.00034 |       0.57618\n",
      "Evaluating losses...\n",
      "     -0.00239 |      -0.00576 |     124.23669 |       0.00033 |       0.57623\n",
      "-----------------------------------\n",
      "| EpLenMean       | 75.6          |\n",
      "| EpRewMean       | 75.6          |\n",
      "| EpThisIter      | 1             |\n",
      "| EpisodesSoFar   | 86            |\n",
      "| TimeElapsed     | 10.8          |\n",
      "| TimestepsSoFar  | 8262          |\n",
      "| ev_tdlam_before | -0.014        |\n",
      "| loss_ent        | 0.5762321     |\n",
      "| loss_kl         | 0.0003347779  |\n",
      "| loss_pol_entpen | -0.005762321  |\n",
      "| loss_pol_surr   | -0.0023898147 |\n",
      "| loss_vf_loss    | 124.236694    |\n",
      "-----------------------------------\n",
      "********** Iteration 26 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00011 |      -0.00587 |     117.93562 |      1.13e-06 |       0.58732\n",
      "     3.18e-05 |      -0.00587 |     117.43345 |      3.44e-06 |       0.58738\n",
      "     9.44e-05 |      -0.00588 |     117.00997 |      3.49e-06 |       0.58776\n",
      "    -1.15e-05 |      -0.00588 |     116.59540 |      4.29e-06 |       0.58829\n",
      "Evaluating losses...\n",
      "    -9.82e-05 |      -0.00588 |     116.36038 |      4.90e-06 |       0.58844\n",
      "------------------------------------\n",
      "| EpLenMean       | 77.6           |\n",
      "| EpRewMean       | 77.6           |\n",
      "| EpThisIter      | 2              |\n",
      "| EpisodesSoFar   | 88             |\n",
      "| TimeElapsed     | 11.2           |\n",
      "| TimestepsSoFar  | 8674           |\n",
      "| ev_tdlam_before | 0.0023         |\n",
      "| loss_ent        | 0.58844006     |\n",
      "| loss_kl         | 4.9049568e-06  |\n",
      "| loss_pol_entpen | -0.0058844006  |\n",
      "| loss_pol_surr   | -9.8228455e-05 |\n",
      "| loss_vf_loss    | 116.36038      |\n",
      "------------------------------------\n",
      "********** Iteration 27 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00031 |      -0.00591 |     126.96896 |      1.03e-06 |       0.59112\n",
      "     -0.00191 |      -0.00591 |     126.46227 |      6.18e-05 |       0.59050\n",
      "     -0.00150 |      -0.00590 |     125.92432 |       0.00026 |       0.58952\n",
      "     -0.00141 |      -0.00590 |     125.37229 |       0.00027 |       0.58952\n",
      "Evaluating losses...\n",
      "     -0.00192 |      -0.00590 |     125.03140 |       0.00018 |       0.59000\n",
      "-----------------------------------\n",
      "| EpLenMean       | 78.6          |\n",
      "| EpRewMean       | 78.6          |\n",
      "| EpThisIter      | 1             |\n",
      "| EpisodesSoFar   | 89            |\n",
      "| TimeElapsed     | 11.5          |\n",
      "| TimestepsSoFar  | 9015          |\n",
      "| ev_tdlam_before | -0.000978     |\n",
      "| loss_ent        | 0.58999795    |\n",
      "| loss_kl         | 0.00018295583 |\n",
      "| loss_pol_entpen | -0.0058999793 |\n",
      "| loss_pol_surr   | -0.0019204654 |\n",
      "| loss_vf_loss    | 125.0314      |\n",
      "-----------------------------------\n",
      "********** Iteration 28 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00016 |      -0.00591 |     132.59378 |      1.08e-05 |       0.59123\n",
      "     -0.00076 |      -0.00592 |     132.27095 |      7.42e-05 |       0.59222\n",
      "     -0.00101 |      -0.00593 |     131.99123 |      8.47e-05 |       0.59317\n",
      "     -0.00097 |      -0.00594 |     131.70673 |      6.19e-05 |       0.59393\n",
      "Evaluating losses...\n",
      "     -0.00103 |      -0.00594 |     131.53409 |      6.47e-05 |       0.59427\n",
      "-----------------------------------\n",
      "| EpLenMean       | 81.2          |\n",
      "| EpRewMean       | 81.2          |\n",
      "| EpThisIter      | 2             |\n",
      "| EpisodesSoFar   | 91            |\n",
      "| TimeElapsed     | 11.9          |\n",
      "| TimestepsSoFar  | 9446          |\n",
      "| ev_tdlam_before | -0.000638     |\n",
      "| loss_ent        | 0.59427094    |\n",
      "| loss_kl         | 6.469127e-05  |\n",
      "| loss_pol_entpen | -0.0059427093 |\n",
      "| loss_pol_surr   | -0.0010312144 |\n",
      "| loss_vf_loss    | 131.53409     |\n",
      "-----------------------------------\n",
      "********** Iteration 29 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -3.17e-05 |      -0.00594 |     127.46445 |      1.74e-07 |       0.59392\n",
      "     -0.00048 |      -0.00594 |     127.25182 |      2.88e-06 |       0.59403\n",
      "     -0.00057 |      -0.00594 |     127.01543 |      2.03e-05 |       0.59396\n",
      "     -0.00060 |      -0.00594 |     126.78362 |      2.93e-05 |       0.59426\n",
      "Evaluating losses...\n",
      "     -0.00076 |      -0.00595 |     126.63162 |      2.33e-05 |       0.59459\n",
      "-----------------------------------\n",
      "| EpLenMean       | 81.4          |\n",
      "| EpRewMean       | 81.4          |\n",
      "| EpThisIter      | 1             |\n",
      "| EpisodesSoFar   | 92            |\n",
      "| TimeElapsed     | 12.3          |\n",
      "| TimestepsSoFar  | 9735          |\n",
      "| ev_tdlam_before | -0.00396      |\n",
      "| loss_ent        | 0.59458506    |\n",
      "| loss_kl         | 2.3297423e-05 |\n",
      "| loss_pol_entpen | -0.0059458506 |\n",
      "| loss_pol_surr   | -0.0007633697 |\n",
      "| loss_vf_loss    | 126.63162     |\n",
      "-----------------------------------\n",
      "********** Iteration 30 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     3.29e-06 |      -0.00617 |     133.69823 |      9.35e-08 |       0.61711\n",
      "     4.02e-05 |      -0.00617 |     133.60722 |      5.57e-07 |       0.61724\n",
      "     4.06e-05 |      -0.00617 |     133.51961 |      6.31e-07 |       0.61733\n",
      "     2.40e-05 |      -0.00617 |     133.43893 |      6.27e-07 |       0.61738\n",
      "Evaluating losses...\n",
      "     3.81e-06 |      -0.00617 |     133.38467 |      5.20e-07 |       0.61738\n",
      "-----------------------------------\n",
      "| EpLenMean       | 83.7          |\n",
      "| EpRewMean       | 83.7          |\n",
      "| EpThisIter      | 2             |\n",
      "| EpisodesSoFar   | 94            |\n",
      "| TimeElapsed     | 12.7          |\n",
      "| TimestepsSoFar  | 10179         |\n",
      "| ev_tdlam_before | 0.00305       |\n",
      "| loss_ent        | 0.61738265    |\n",
      "| loss_kl         | 5.1978327e-07 |\n",
      "| loss_pol_entpen | -0.0061738263 |\n",
      "| loss_pol_surr   | 3.810972e-06  |\n",
      "| loss_vf_loss    | 133.38467     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo1.pposgd_simple.PPO1 at 0x7faeb0acb400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines import PPO1\n",
    "\n",
    "model = PPO1('MlpPolicy', 'CartPole-v0', verbose=1, tensorboard_log=\"log/ppo/\")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines Implementation of TRPO\n",
    "\n",
    "From the stable-baselines repo to compare my implementation with the baselines implementation of TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating environment from the given name, wrapped in a DummyVecEnv.\n",
      "********** Iteration 0 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.518 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.109 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0187          0\n",
      "         1   4.86e-05      0.104\n",
      "         2   3.31e-05      0.346\n",
      "         3   2.52e-05      0.363\n",
      "         4   1.08e-07       0.49\n",
      "         5   1.79e-06       0.49\n",
      "         6    1.1e-09      0.492\n",
      "         7   2.73e-11      0.492\n",
      "\u001b[35mdone in 0.154 seconds\u001b[0m\n",
      "Expected: 0.019 Actual: 0.018\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.076 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 21.1        |\n",
      "| EpRewMean       | 21.1        |\n",
      "| EpThisIter      | 48          |\n",
      "| EpisodesSoFar   | 48          |\n",
      "| TimeElapsed     | 0.936       |\n",
      "| TimestepsSoFar  | 1024        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.6851318   |\n",
      "| ev_tdlam_before | -0.00177    |\n",
      "| meankl          | 0.008057453 |\n",
      "| optimgain       | 0.018265154 |\n",
      "| surrgain        | 0.018265154 |\n",
      "---------------------------------\n",
      "********** Iteration 1 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.475 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.000 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0228          0\n",
      "         1   0.000535      0.111\n",
      "         2   0.000689      0.139\n",
      "         3   9.11e-05      0.513\n",
      "         4   1.49e-06      0.739\n",
      "         5   3.16e-07      0.747\n",
      "         6   9.79e-07      0.747\n",
      "         7   5.03e-08       0.75\n",
      "         8   1.58e-08       0.75\n",
      "         9   6.82e-11       0.75\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.024 Actual: 0.028\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.050 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 23.6        |\n",
      "| EpRewMean       | 23.6        |\n",
      "| EpThisIter      | 42          |\n",
      "| EpisodesSoFar   | 90          |\n",
      "| TimeElapsed     | 1.5         |\n",
      "| TimestepsSoFar  | 2050        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.663458    |\n",
      "| ev_tdlam_before | 0.00491     |\n",
      "| meankl          | 0.009241743 |\n",
      "| optimgain       | 0.027608946 |\n",
      "| surrgain        | 0.027608946 |\n",
      "---------------------------------\n",
      "********** Iteration 2 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.562 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0479          0\n",
      "         1    0.00356      0.145\n",
      "         2    0.00186      0.274\n",
      "         3   0.000362      0.316\n",
      "         4   3.16e-05      0.555\n",
      "         5   8.82e-06      0.686\n",
      "         6   2.23e-06       0.73\n",
      "         7   2.67e-07       0.73\n",
      "         8   9.44e-07      0.731\n",
      "         9   1.17e-07      0.732\n",
      "        10    3.1e-08      0.733\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.033 Actual: 0.032\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.062 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 34.8        |\n",
      "| EpRewMean       | 34.8        |\n",
      "| EpThisIter      | 27          |\n",
      "| EpisodesSoFar   | 117         |\n",
      "| TimeElapsed     | 2.16        |\n",
      "| TimestepsSoFar  | 3077        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.62818563  |\n",
      "| ev_tdlam_before | -0.00148    |\n",
      "| meankl          | 0.00975647  |\n",
      "| optimgain       | 0.032262627 |\n",
      "| surrgain        | 0.032262627 |\n",
      "---------------------------------\n",
      "********** Iteration 3 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.471 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0275          0\n",
      "         1    0.00193     0.0859\n",
      "         2    0.00281      0.199\n",
      "         3   0.000783      0.306\n",
      "         4   5.44e-05      0.375\n",
      "         5   1.91e-05      0.523\n",
      "         6   3.94e-06      0.597\n",
      "         7   1.15e-05      0.598\n",
      "         8   1.75e-07      0.628\n",
      "         9   7.78e-07      0.629\n",
      "        10   6.56e-08       0.63\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.025 Actual: 0.026\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.052 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 49.2        |\n",
      "| EpRewMean       | 49.2        |\n",
      "| EpThisIter      | 15          |\n",
      "| EpisodesSoFar   | 132         |\n",
      "| TimeElapsed     | 2.71        |\n",
      "| TimestepsSoFar  | 4113        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.6076422   |\n",
      "| ev_tdlam_before | -0.00709    |\n",
      "| meankl          | 0.010938769 |\n",
      "| optimgain       | 0.025824588 |\n",
      "| surrgain        | 0.025824588 |\n",
      "---------------------------------\n",
      "********** Iteration 4 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.457 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.000 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0502          0\n",
      "         1    0.00177      0.137\n",
      "         2   0.000947      0.209\n",
      "         3   0.000252       0.22\n",
      "         4   0.000142      0.431\n",
      "         5   4.12e-05      0.673\n",
      "         6   2.53e-06      0.787\n",
      "         7   3.17e-05       0.79\n",
      "         8   6.98e-06      0.798\n",
      "         9   1.22e-06      0.798\n",
      "        10   1.01e-07      0.803\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.030 Actual: 0.026\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.043 seconds\u001b[0m\n",
      "----------------------------------\n",
      "| EpLenMean       | 59.7         |\n",
      "| EpRewMean       | 59.7         |\n",
      "| EpThisIter      | 15           |\n",
      "| EpisodesSoFar   | 147          |\n",
      "| TimeElapsed     | 3.26         |\n",
      "| TimestepsSoFar  | 5172         |\n",
      "| entloss         | 0.0          |\n",
      "| entropy         | 0.5816072    |\n",
      "| ev_tdlam_before | -0.0317      |\n",
      "| meankl          | 0.0078895725 |\n",
      "| optimgain       | 0.025714602  |\n",
      "| surrgain        | 0.025714602  |\n",
      "----------------------------------\n",
      "********** Iteration 5 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.481 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.007 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0144          0\n",
      "         1    0.00743     0.0554\n",
      "         2   0.000411     0.0839\n",
      "         3   0.000433      0.119\n",
      "         4   9.47e-05      0.267\n",
      "         5   2.59e-05      0.525\n",
      "         6   0.000196      0.558\n",
      "         7    6.2e-06      0.564\n",
      "         8   5.74e-06        0.6\n",
      "         9   6.99e-07      0.601\n",
      "        10   2.18e-07      0.608\n",
      "\u001b[35mdone in 0.017 seconds\u001b[0m\n",
      "Expected: 0.018 Actual: 0.015\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.043 seconds\u001b[0m\n",
      "----------------------------------\n",
      "| EpLenMean       | 77.5         |\n",
      "| EpRewMean       | 77.5         |\n",
      "| EpThisIter      | 7            |\n",
      "| EpisodesSoFar   | 154          |\n",
      "| TimeElapsed     | 3.83         |\n",
      "| TimestepsSoFar  | 6219         |\n",
      "| entloss         | 0.0          |\n",
      "| entropy         | 0.58555114   |\n",
      "| ev_tdlam_before | -0.163       |\n",
      "| meankl          | 0.006005341  |\n",
      "| optimgain       | 0.0151105225 |\n",
      "| surrgain        | 0.0151105225 |\n",
      "----------------------------------\n",
      "********** Iteration 6 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.450 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.000 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0254          0\n",
      "         1    0.00258     0.0405\n",
      "         2   0.000755      0.068\n",
      "         3   0.000164     0.0771\n",
      "         4    9.8e-05      0.303\n",
      "         5   2.57e-05      0.457\n",
      "         6   0.000121      0.523\n",
      "         7   6.51e-06      0.527\n",
      "         8   2.27e-05      0.556\n",
      "         9   2.44e-06      0.558\n",
      "        10   1.08e-06      0.572\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.017 Actual: 0.017\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.046 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 89.8        |\n",
      "| EpRewMean       | 89.8        |\n",
      "| EpThisIter      | 7           |\n",
      "| EpisodesSoFar   | 161         |\n",
      "| TimeElapsed     | 4.37        |\n",
      "| TimestepsSoFar  | 7334        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.5801389   |\n",
      "| ev_tdlam_before | -0.0966     |\n",
      "| meankl          | 0.010764189 |\n",
      "| optimgain       | 0.016814295 |\n",
      "| surrgain        | 0.016814295 |\n",
      "---------------------------------\n",
      "********** Iteration 7 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.474 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.000 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0     0.0353          0\n",
      "         1    0.00203     0.0259\n",
      "         2    0.00142      0.111\n",
      "         3   0.000631      0.167\n",
      "         4   0.000177      0.209\n",
      "         5   4.24e-05      0.418\n",
      "         6   0.000234      0.422\n",
      "         7   9.05e-06      0.485\n",
      "         8   7.66e-07      0.516\n",
      "         9   5.46e-06      0.526\n",
      "        10   2.45e-06      0.526\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.018 Actual: 0.012\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.053 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 106         |\n",
      "| EpRewMean       | 106         |\n",
      "| EpThisIter      | 6           |\n",
      "| EpisodesSoFar   | 167         |\n",
      "| TimeElapsed     | 4.94        |\n",
      "| TimestepsSoFar  | 8409        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.5758752   |\n",
      "| ev_tdlam_before | -0.297      |\n",
      "| meankl          | 0.004431309 |\n",
      "| optimgain       | 0.012316832 |\n",
      "| surrgain        | 0.012316832 |\n",
      "---------------------------------\n",
      "********** Iteration 8 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.473 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0    0.00336          0\n",
      "         1    0.00403     0.0268\n",
      "         2    0.00272      0.187\n",
      "         3   0.000411      0.244\n",
      "         4   0.000919      0.283\n",
      "         5   3.88e-05      0.587\n",
      "         6   2.08e-05      0.643\n",
      "         7   1.11e-05      0.643\n",
      "         8   2.61e-06       0.69\n",
      "         9   2.58e-05      0.701\n",
      "        10   1.25e-07      0.705\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.020 Actual: 0.019\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.040 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 124         |\n",
      "| EpRewMean       | 124         |\n",
      "| EpThisIter      | 5           |\n",
      "| EpisodesSoFar   | 172         |\n",
      "| TimeElapsed     | 5.48        |\n",
      "| TimestepsSoFar  | 9542        |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.58651465  |\n",
      "| ev_tdlam_before | -0.385      |\n",
      "| meankl          | 0.010802735 |\n",
      "| optimgain       | 0.018791882 |\n",
      "| surrgain        | 0.018791882 |\n",
      "---------------------------------\n",
      "********** Iteration 9 ************\n",
      "Optimizing Policy...\n",
      "\u001b[35msampling\u001b[0m\n",
      "\u001b[35mdone in 0.477 seconds\u001b[0m\n",
      "\u001b[35mcomputegrad\u001b[0m\n",
      "\u001b[35mdone in 0.000 seconds\u001b[0m\n",
      "\u001b[35mcg\u001b[0m\n",
      "      iter residual norm  soln norm\n",
      "         0   0.000924          0\n",
      "         1    0.00196     0.0243\n",
      "         2    0.00132     0.0662\n",
      "         3    0.00159      0.326\n",
      "         4   0.000122      0.503\n",
      "         5   2.29e-05      0.634\n",
      "         6    0.00035      0.682\n",
      "         7    6.9e-06      0.691\n",
      "         8   4.69e-06      0.714\n",
      "         9   1.82e-05      0.717\n",
      "        10    1.8e-07      0.729\n",
      "\u001b[35mdone in 0.016 seconds\u001b[0m\n",
      "Expected: 0.017 Actual: 0.009\n",
      "Stepsize OK!\n",
      "\u001b[35mvf\u001b[0m\n",
      "\u001b[35mdone in 0.051 seconds\u001b[0m\n",
      "---------------------------------\n",
      "| EpLenMean       | 138         |\n",
      "| EpRewMean       | 138         |\n",
      "| EpThisIter      | 7           |\n",
      "| EpisodesSoFar   | 179         |\n",
      "| TimeElapsed     | 6.03        |\n",
      "| TimestepsSoFar  | 10758       |\n",
      "| entloss         | 0.0         |\n",
      "| entropy         | 0.5784534   |\n",
      "| ev_tdlam_before | -0.316      |\n",
      "| meankl          | 0.008309755 |\n",
      "| optimgain       | 0.008746282 |\n",
      "| surrgain        | 0.008746282 |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.trpo_mpi.trpo_mpi.TRPO at 0x1e422e929e8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines import TRPO\n",
    "\n",
    "model = TRPO('MlpPolicy', 'CartPole-v0', verbose=1, tensorboard_log=\"log/trpo/\")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Tensorboard for plots\n",
    "\n",
    "1. Change directory into this project directory\n",
    "2. Execute the following command\n",
    "    `tensorboard --logdir=log/`\n",
    "3. Visit the localhost page with the provided port number to monitor tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
